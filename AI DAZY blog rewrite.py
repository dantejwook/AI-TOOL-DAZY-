# AI DAZY TEST MODE

# ê¸°ë³¸ ì˜ì—­ ----------------------------------------------------------------------------------------------------------------------------------------------------

import streamlit as st
import zipfile
import os
import openai
import json
import hashlib
import re
import shutil
import secrets
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from pathlib import Path
from hdbscan import HDBSCAN
import numpy as np


# ============================
# ğŸ”§ Recursive Split Settings
# ============================
MAX_FILES_PER_CLUSTER = 25
MAX_RECURSION_DEPTH = 2

# ============================
# ğŸ” Token Store (Server Memory)
# ============================
TOKEN_STORE = {}
TOKEN_EXPIRE_HOURS = 3

# ------------------------------------------
# ğŸŒˆ ê¸°ë³¸ í˜ì´ì§€ ì„¤ì •
# ------------------------------------------
st.set_page_config(
    page_title="AI dazy test mode",
    page_icon="ğŸ—‚ï¸",
    layout="wide",
)
# ============================
# ğŸ”’ Password + Token Gate
# ============================
APP_PASSWORD = st.secrets.get("APP_PASSWORD") or os.getenv("APP_PASSWORD")

params = st.experimental_get_query_params()
token = params.get("auth", [None])[0]

if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

# í† í° ìˆìœ¼ë©´ ì¸ì¦ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
if token:
    st.session_state.authenticated = True

# ë¹„ì¸ì¦ ìƒíƒœ â†’ ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
if not st.session_state.authenticated:
    st.markdown("<br><br><br><br>", unsafe_allow_html=True)
    col = st.columns([1, 2, 1])[1]

    with col:
        st.markdown(
            """
            <div style="
                background:var(--secondary-background-color);
                padding:2rem;
                border-radius:16px;
                text-align:center;
                color:var(--text-color);">
                <h2>ğŸ”’ Access Password</h2>
                <p>ì´ ì•±ì€ ì œí•œëœ ì‚¬ìš©ìë§Œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
            </div>
            """,
            unsafe_allow_html=True,
        )

        pw = st.text_input("Password", type="password", label_visibility="collapsed")

        if pw:
            if pw == APP_PASSWORD:
                new_token = secrets.token_hex(16)
                st.experimental_set_query_params(auth=new_token)
                st.session_state.authenticated = True
                st.success("ì ‘ê·¼ í—ˆìš©")
                st.rerun()
            else:
                st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    st.stop()

# ============================
# ğŸ”‘ API Key Input (First Time)
# ============================
if "api_key" not in st.session_state:
    st.markdown("### ğŸ”‘ OpenAI API Key")
 
    api_key_input = st.text_input(
        "OpenAI API Key",
        type="password",
        placeholder="sk-xxxxxxxxxxxxxxxxxxxxxxxx",
        label_visibility="collapsed",
    )
    st.caption("1ï¸âƒ£ í•´ë‹¹ì•±ì€ chat gpt / openaië¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ")
    st.caption("2ï¸âƒ£ openai ì—ì„œ ë°œê¸‰í•œ api key ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
    st.caption("3ï¸âƒ£ api key ë°œê¸‰ ë°›ê¸° : [ https://openai.com/ko-KR/api/ ]")
    
    if api_key_input:
        try:
            openai.api_key = api_key_input
            openai.Model.list()  # ìœ íš¨ì„± ê²€ì‚¬

            TOKEN_STORE[token] = {
                "api_key": api_key_input,
                "expires_at": datetime.utcnow() + timedelta(hours=TOKEN_EXPIRE_HOURS),
            }

            st.session_state.api_key = api_key_input
            st.success("API Key ì¸ì¦ ì™„ë£Œ")
            st.rerun()
        except Exception:
            st.error("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤.")

    st.stop()

# ============================
# ğŸ“ File Uploader State (ì´ˆê¸° 1íšŒ)
# ============================
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = 0

# ============================
# ğŸ¨ ìŠ¤íƒ€ì¼
# ============================
st.markdown(
"""
<style>

/* =========================
   ì•± ê¸°ë³¸ ë°°ê²½
========================= */
body {
    background-color: var(--background-color);
    font-family: 'Pretendard', sans-serif;
}

/* =========================
   ë²„íŠ¼ ìŠ¤íƒ€ì¼
========================= */
.stButton>button {
    border-radius: 10px;
    background-color: var(--primary-color);
    color: var(--text-color);

    /* ë°ì€ ë°°ê²½ì—ì„œ ê°€ë…ì„± í™•ë³´ */
    text-shadow: 0 1px 1px rgba(0,0,0,0.15);
    
    border: none;
    padding: 0.6em 1.2em;
    font-weight: 600;

    /* ë²„íŠ¼ ì „ìš© ê·¸ë¦¼ì */
    box-shadow: 0 4px 10px rgba(0, 0, 0, 0.18);

    transition:
        transform 0.15s ease,
        box-shadow 0.15s ease,
        filter 0.15s ease;
}

.stButton>button:hover {
    transform: translateY(-1px);
    box-shadow: 0 6px 14px rgba(0, 0, 0, 0.22);
    filter: brightness(0.97);
}

.stButton>button:active {
    transform: translateY(0);
    box-shadow: 0 3px 6px rgba(0, 0, 0, 0.25);
}

/* =========================
   ìƒíƒœë°”
========================= */
.status-bar {
    background-color: var(--secondary-background-color);
    color: var(--text-color);
    border-radius: 6px;
    padding: 0.5em;
    margin-top: 10px;
    font-size: 0.9em;

    /* ë²„íŠ¼ì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ëŠ” ìš”ì†Œ ì œê±° */
    box-shadow: none;
    border: none;
}

/* =========================
   ë¡œê·¸ ë°•ìŠ¤ (ì¹´ë“œ ìœ ì§€)
========================= */
.log-box {
    background-color: #dbede6;
    color: #050505;
    padding: 0.8em;
    margin-top: 10px;
    height: 120px;
    overflow-y: auto;
    font-size: 0.85em;

    /* ë°˜ì‘í˜• */
    border-radius: 12px;

    /* âŒ border ì œê±° */
    border: none;

    /* âœ… Streamlit ëŒ€ì‘ ìœ¤ê³½ */
    outline-offset: -1px;
    box-shadow: none;
}

/* =========================
   í…Œë§ˆ ë¯¸ì„¸ ì¡°ì •(ìƒíƒœë°” ì œì™¸)
========================= */
@media (prefers-color-scheme: dark) {
    .log-box {
        outline: 1.5px solid rgba(255, 255, 255, 0.16);
    }
}

@media (prefers-color-scheme: light) {
    .log-box {
        outline: 1.5px solid rgba(0, 0, 0, 0.28);
    }
}

</style>
""",
unsafe_allow_html=True,
)

# ============================
# ì‚¬ì´ë“œë°” ì„¤ì • ë¶€ë¶„
# ============================

# ------------------------------------------
# ìºì‹œ
# ------------------------------------------
CACHE_DIR = Path(".cache")
CACHE_DIR.mkdir(exist_ok=True)

def load_cache(p):
    try:
        return json.loads(p.read_text(encoding="utf-8")) if p.exists() else {}
    except Exception:
        return {}

def save_cache(p, d):
    p.write_text(json.dumps(d, ensure_ascii=False, indent=2), encoding="utf-8")

EMBED_CACHE = CACHE_DIR / "embeddings.json"
GROUP_CACHE = CACHE_DIR / "group_names.json"
README_CACHE = CACHE_DIR / "readmes.json"
EXPAND_CACHE = CACHE_DIR / "expands.json"

embedding_cache = load_cache(EMBED_CACHE)
group_cache = load_cache(GROUP_CACHE)
readme_cache = load_cache(README_CACHE)
expand_cache = load_cache(EXPAND_CACHE)

def reset_cache():
    if CACHE_DIR.exists():
        shutil.rmtree(CACHE_DIR)
    CACHE_DIR.mkdir(exist_ok=True)
    embedding_cache.clear()
    group_cache.clear()
    readme_cache.clear()
    expand_cache.clear()

def reset_output():
    output_dir = Path("output_docs")
    zip_path = Path("result_documents.zip")

    if output_dir.exists():
        shutil.rmtree(output_dir)
    if zip_path.exists():
        zip_path.unlink()

st.sidebar.markdown(
    """

"""
)

# ============================
#  ì‚¬ì´ë“œë°” UI
# ============================

# ------------------------------------------
# âœ… API Session Active (Sidebar)
# ------------------------------------------
openai.api_key = st.session_state.api_key

with st.sidebar:
    st.success("API ì¸ì¦ ì„±ê³µ")

# ------------------------------------------
# ğŸ”’ Logout Button
# ------------------------------------------
st.sidebar.title("âš™ï¸ Setting")
col1, col2 = st.sidebar.columns([1, 1], gap="small")

with col1:
    if st.button("API Key ë³€ê²½", use_container_width=True):
        st.session_state.pop("api_key", None)
        st.rerun()

with col2:
    if st.button("ë¡œê·¸ì•„ì›ƒ", use_container_width=True):
    # ì¸ì¦ ìƒíƒœ ì œê±°
        st.session_state.pop("authenticated", None)
        st.session_state.pop("api_key", None)

    # URL í† í° ì œê±°
        st.experimental_set_query_params()

    # ì „ì²´ ë¦¬ì…‹
        st.rerun()

st.sidebar.markdown("### ğŸ’¡ ì‚¬ìš© íŒ")
st.sidebar.markdown(
    """
- ğŸ“ íŒŒì¼ì„ **ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ ì‹œì‘** ë©ë‹ˆë‹¤.
- ğŸ“‚ **ì—¬ëŸ¬ ë¬¸ì„œë¥¼ í•œ ë²ˆì— ì—…ë¡œë“œ**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ğŸ§  ë¬¸ì„œëŠ” **AIê°€ ìë™ìœ¼ë¡œ ì£¼ì œë³„ ë¶„ë¥˜**í•©ë‹ˆë‹¤.
- ğŸ“ í´ë” ìˆ˜ê°€ ë§ìœ¼ë©´ **ìë™ìœ¼ë¡œ í•˜ìœ„ í´ë”ë¡œ ë¶„í•´**ë©ë‹ˆë‹¤.
- â³ ë¬¸ì„œ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ì²˜ë¦¬ ì‹œê°„ì´ ëŠ˜ì–´ë‚©ë‹ˆë‹¤.
- ğŸ“¦ ì™„ë£Œ í›„ **ZIP íŒŒì¼ë¡œ í•œ ë²ˆì— ë‹¤ìš´ë¡œë“œ**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
) 

# ============================
# ğŸ“ ë©”ì¸ UI
# ============================
left_col, right_col = st.columns([1, 1])

st.subheader("AI auto file analyzer")
st.caption("ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ìë™ìœ¼ë¡œ êµ¬ì¡°í™”í•©ë‹ˆë‹¤")

with left_col:
    st.subheader("File upload")
    uploaded_files = st.file_uploader(
        "ğŸ“ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (.md, .pdf, .txt)",
        accept_multiple_files=True,
        type=["md", "pdf", "txt"],
        key=f"uploader_{st.session_state.uploader_key}",
    )
    if st.button("Upload File Reset", use_container_width=True):
        st.session_state.uploader_key += 1
        st.rerun()
    # âœ… ë°˜ë“œì‹œ ì—¬ê¸° ì•ˆì—ì„œ
    col2, col3 = st.columns([1, 1], gap="small")

    with col2:
        if st.button("Cache Reset", use_container_width=True):
            reset_cache()
            st.rerun()
            
    with col3:
        if st.button("Download Reset", use_container_width=True):
            reset_output()
            st.rerun()


with right_col:
    st.subheader("ZIP Download")
    st.caption("ğŸ“ ë¬¸ì„œ ì •ë¦¬ í›„ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì´ í™œì„±í™” ë©ë‹ˆë‹¤.")

    zip_placeholder = st.empty()   # ğŸ‘ˆ ìœ„ì— ë‘ê³ 

# ============================
# âš™ï¸ ìƒíƒœ / ë¡œê·¸
# ============================
progress_placeholder = st.empty()
progress_text = st.empty()
log_box = st.empty()
logs = []

def log(msg):
    logs.append(msg)
    log_box.markdown(
        "<div class='log-box'>" + "<br>".join(logs[-10:]) + "</div>",
        unsafe_allow_html=True,
    )

def h(t: str):
    return hashlib.sha256(t.encode("utf-8")).hexdigest()

# ============================
# ğŸ“Š ìƒíƒœë°” ì—…ë°ì´íŠ¸ í—¬í¼
# ============================
def update_progress(pct: int, msg: str):
    """ìƒíƒœë°” + ë¡œê·¸ ë™ì‹œì— ì—…ë°ì´íŠ¸"""
    try:
        pct = max(0, min(100, int(pct)))  # 0~100 ì‚¬ì´ë¡œ ì œí•œ
        progress_placeholder.progress(pct)
        progress_text.markdown(
            f"<div class='status-bar'>| {msg} | [ {pct}% ]</div>",
            unsafe_allow_html=True
        )
        log(msg)
    except Exception as e:
        st.warning(f"âš ï¸ ìƒíƒœ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")


# ê¸°ë³¸ ì˜ì—­ ----------------------------------------------------------------------------------------------------------------------------------------------------

# ê¸°ëŠ¥ ì˜ì—­ ----------------------------------------------------------------------------------------------------------------------------------------------------

# ============================
# âœ¨ ìœ í‹¸ (íŒŒì¼/ìºì‹œ í•¨ìˆ˜)
# ============================
def sanitize_folder_name(name: str) -> str:
    """í´ë”/íŒŒì¼ ì´ë¦„ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ê³  ì•ˆì „í•œ ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
    name = (name or "").strip()
    name = re.sub(r"[^\wê°€-í£\s]", "", name)
    name = re.sub(r"\s+", "_", name)
    return name.strip("_") or "ê¸°íƒ€_ë¬¸ì„œ"
    
def title_from_filename(file_name: str) -> str:
    """íŒŒì¼ ì´ë¦„ì—ì„œ í™•ì¥ìë¥¼ ì œê±°í•˜ê³ , ë°‘ì¤„/í•˜ì´í”ˆ ë“±ì„ ê³µë°±ìœ¼ë¡œ ë°”ê¾¼ ì œëª© ë¬¸ìì—´ ë°˜í™˜"""
    base = file_name.rsplit(".", 1)[0]
    base = re.sub(r"[_\\-]+", " ", base)
    base = re.sub(r"\\s+", " ", base).strip()
    return base

def embed_texts(texts, batch_size=50):
    """ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ OpenAI ì„ë² ë”© APIë¡œ ë³€í™˜ (ëŒ€ìš©ëŸ‰ ì•ˆì „)"""
    results = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        missing = [t for t in batch if h(t) not in embedding_cache]

        if missing:
            try:
                r = openai.Embedding.create(
                    model="text-embedding-3-large",
                    input=missing,
                )
                for t, d in zip(missing, r["data"]):
                    embedding_cache[h(t)] = d["embedding"]
                save_cache(EMBED_CACHE, embedding_cache)
            except Exception as e:
                st.error(f"âŒ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ (batch {i//batch_size+1}): {e}")
                continue

        results.extend([embedding_cache[h(t)] for t in batch])

    return results

def load_category_structure(readme_file):
    text = readme_file.getvalue().decode("utf-8")
    prompt = f"""
ë‹¤ìŒì€ ë¸”ë¡œê·¸ ì¹´í…Œê³ ë¦¬ ë° ì„¸ë¶€ ì£¼ì œ ì •ë¦¬ ë¬¸ì„œì…ë‹ˆë‹¤.
ì´ ë¬¸ì„œë¥¼ JSON íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ë³€í™˜í•˜ì„¸ìš”.

ì¶œë ¥ ì˜ˆì‹œ:
[
  {{"category": "ì‹œì¥ ì´í•´ & íŠ¸ë Œë“œ", "subtopics": ["ë·°í‹°ì—…ê³„ ì‚°ì—… íŠ¸ë Œë“œ", "êµ­ë‚´ ë·°í‹°ì—…ê³„ íŠ¸ë Œë“œ ë³€í™”"]}},
  {{"category": "êµ­ë‚´ì™¸ ë·°í‹°ì—…ê³„ í•«ì´ìŠˆ", "subtopics": ["ì •ì±…Â·ê·œì œÂ·ì‹œì¥ ì´ìŠˆ"]}}
]
"""

    r = openai.ChatCompletion.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ë¬¸ì„œë¥¼ JSON êµ¬ì¡°ë¡œ íŒŒì‹±í•˜ëŠ” ì „ë¬¸ê°€ë‹¤."},
            {"role": "user", "content": prompt + "\n" + text}
        ],
        temperature=0
    )

    try:
        return json.loads(r["choices"][0]["message"]["content"])
    except Exception:
        st.error("ì¹´í…Œê³ ë¦¬ êµ¬ì¡°ë¥¼ íŒŒì‹±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        return []

# ============================
# ğŸ“˜ README ê¸°ë°˜ í´ë” ìƒì„± (ì„ íƒ)
# ============================

def create_category_folders(base_dir, category_structure):
    folder_map = {}
    for cat in category_structure:
        cat_folder = base_dir / f"{sanitize_folder_name(cat['category'])}"
        cat_folder.mkdir(exist_ok=True)
        sub_map = {}
        for sub in cat.get("subtopics", []):
            sub_folder = cat_folder / sanitize_folder_name(sub)
            sub_folder.mkdir(exist_ok=True)
            sub_map[sub] = sub_folder
        folder_map[cat['category']] = sub_map
    return folder_map

# ============================
# ğŸ§  ë¬¸ì„œ í™•ì¥ + ì„ë² ë”© í†µí•©
# ============================

def embed_texts(texts, batch_size=40):
    """ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ OpenAI ì„ë² ë”© APIë¡œ ë³€í™˜ (ëŒ€ìš©ëŸ‰/í† í° ì œí•œ ì•ˆì „ ë²„ì „)"""
    results = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        missing = [t for t in batch if h(t) not in embedding_cache]

        if missing:
            try:
                # ê° batchë³„ ì„ë² ë”© ìš”ì²­
                r = openai.Embedding.create(
                    model="text-embedding-3-large",
                    input=missing,
                )
                for t, d in zip(missing, r["data"]):
                    embedding_cache[h(t)] = d["embedding"]

                # âœ… ìºì‹œ ì €ì¥
                save_cache(EMBED_CACHE, embedding_cache)
                log(f"ğŸ§© ì„ë² ë”© batch {i//batch_size + 1} ì™„ë£Œ ({len(batch)}ê°œ)")
            except Exception as e:
                st.error(f"âŒ ì„ë² ë”© batch {i//batch_size + 1} ì˜¤ë¥˜: {e}")
                continue

        # ìºì‹œëœ ë²¡í„°ë¥¼ ìˆœì„œëŒ€ë¡œ append
        results.extend([embedding_cache[h(t)] for t in batch])

    return results


def prepare_blog_embeddings(files):
    """ë¸”ë¡œê·¸ ì´ˆì•ˆ ì„ë² ë”© ìƒì„± (ë°©ì–´ ë²„ì „)"""
    texts, file_objs = [], []

    for f in files:
        try:
            text = f.getvalue().decode("utf-8", errors="ignore")
        except Exception:
            st.warning(f"âš ï¸ {f.name} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ â€” ê±´ë„ˆëœ€")
            continue

        title = title_from_filename(f.name)
        clean_text = re.sub(r"\s+", " ", text.strip())[:4000]  # 4000ì ì œí•œ
        texts.append(f"ì œëª©: {title}\në‚´ìš©: {clean_text}")
        file_objs.append(f)

    if not texts:
        st.error("âŒ ì—…ë¡œë“œëœ ë¸”ë¡œê·¸ ì´ˆì•ˆì—ì„œ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {}

    vectors = embed_texts(texts)

    if not vectors or len(vectors) != len(file_objs):
        st.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {len(vectors)} / ê¸°ëŒ€ê°’ {len(file_objs)}")
        return {}

    st.write(f"âœ… ì„ë² ë”© ì™„ë£Œ: {len(vectors)}ê°œ ë¬¸ì„œ ë³€í™˜ë¨.")
    return dict(zip(file_objs, vectors))


# ============================
# ğŸ“¦ í´ëŸ¬ìŠ¤í„°ë§ + ìë™ ì¬ë¶„í•´ (ì¡°ê±´ë¶€)
# ============================

def match_documents_to_categories(embeddings, category_structure):
    """ë¬¸ì„œì™€ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ (ë°©ì–´ + ë””ë²„ê·¸ ë²„ì „)"""

    # âœ… 1ë‹¨ê³„: ì„ë² ë”© ìœ íš¨ì„± ê²€ì‚¬
    if not embeddings or not isinstance(embeddings, dict):
        st.error("âŒ ì„ë² ë”© ë°ì´í„°ê°€ ë¹„ì–´ ìˆê±°ë‚˜ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.write(f"âš™ï¸ embeddings íƒ€ì…: {type(embeddings)} / ê¸¸ì´: {len(embeddings) if embeddings else 0}")
        return {}

    try:
        sample_names = [f.name for f in list(embeddings.keys())[:3]]
        st.write(f"ğŸ“Š ì„ë² ë”© ìƒ˜í”Œ: {sample_names}")
    except Exception:
        st.warning("âš ï¸ ì„ë² ë”© í‚¤ ìƒ˜í”Œ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥)")

    all_topics = []
    for c in category_structure:
        for sub in c.get("subtopics", []):
            all_topics.append((c["category"], sub))

    if not all_topics:
        st.error("âŒ ì¹´í…Œê³ ë¦¬ êµ¬ì¡°ì— subtopicsê°€ ì—†ìŠµë‹ˆë‹¤. README íŒŒì¼ í™•ì¸ í•„ìš”.")
        return {}

    topic_texts = [f"{cat} - {sub}" for cat, sub in all_topics]
    topic_embeddings = embed_texts(topic_texts)

    if not topic_embeddings or len(topic_embeddings) != len(all_topics):
        st.error("âŒ ì¹´í…Œê³ ë¦¬ ì£¼ì œ ì„ë² ë”© ì‹¤íŒ¨.")
        return {}

    # âœ… ì•ˆì „í•˜ê²Œ numpy ë°°ì—´ ìƒì„±
    try:
        doc_vecs = np.array(list(embeddings.values()), dtype=float)
    except Exception as e:
        st.error(f"âŒ ë¬¸ì„œ ì„ë² ë”© ë°°ì—´ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
        return {}

    sim = cosine_similarity(doc_vecs, np.array(topic_embeddings))
    match_results = {cat: {sub: [] for sub in [s for _, s in all_topics if _ == cat]} for cat, _ in all_topics}

    for i, (file_obj, _) in enumerate(embeddings.items()):
        best_idx = int(np.argmax(sim[i]))
        cat, sub = all_topics[best_idx]
        match_results[cat][sub].append(file_obj)

    st.success("âœ… ë¬¸ì„œ-ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì™„ë£Œ.")
    return match_results

# ============================
# âœ¨ GPT í´ë”ëª… / README ìƒì„±
# ============================

def generate_summary_readme(category, subtopic, files):
    file_titles = [title_from_filename(f.name) for f in files]
    file_titles_text = "\n".join(f"- {t}" for t in file_titles)

    prompt = f"""
'{category}' ì¹´í…Œê³ ë¦¬ì˜ '{subtopic}' ì£¼ì œì™€ ê´€ë ¨ëœ ë¸”ë¡œê·¸ ì´ˆì•ˆë“¤ì…ë‹ˆë‹¤.
ì´ ê¸€ë“¤ì˜ ê³µí†µëœ ë°©í–¥ì„±ê³¼ ì‹œë„ˆì§€, ì£¼ì œì  ì—°ê²°ì„±ì„ ë¶„ì„í•˜ê³ 
README ìš”ì•½ íŒŒì¼ì„ ì‘ì„±í•˜ì„¸ìš”.

í˜•ì‹:
# README_{subtopic}

## ğŸ“˜ ì£¼ì œ ê°œìš”
(ì´ ì£¼ì œê°€ ë‹¤ë£¨ëŠ” í•µì‹¬ ë‚´ìš©)

## ğŸ¤ ì‹œë„ˆì§€ & ì—°ê´€ì„±
(íŒŒì¼ë“¤ì´ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€)

## ğŸ¯ ê³µí†µ ëª©í‘œ
(ì´ ì£¼ì œì—ì„œ ì¼ê´€ëœ í•µì‹¬ ëª©í‘œëŠ” ë¬´ì—‡ì¸ì§€)

### í¬í•¨ëœ ë¬¸ì„œ ëª©ë¡
{file_titles_text}
"""

    r = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ë¸”ë¡œê·¸ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ìš”ì•½ë¬¸ì„œë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ë‹¤."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.5,
    )

    return r["choices"][0]["message"]["content"].strip()

# ============================
# ğŸš€ ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ìƒíƒœë°” í¬í•¨)
# ============================

if uploaded_files:
    # ì´ˆê¸° ìƒíƒœ 0%
    update_progress(0, "ëŒ€ê¸° ì¤‘â€¦")

    readme_file = None
    blog_files = []
    for f in uploaded_files:
        if "readme" in f.name.lower():
            readme_file = f
        else:
            blog_files.append(f)

    if not readme_file:
        st.error("ì¹´í…Œê³ ë¦¬ êµ¬ì¡°ê°€ ë‹´ê¸´ README íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        st.stop()

    reset_output()
    output_dir = Path("output_docs")
    output_dir.mkdir(exist_ok=True)

    # ë‹¨ê³„ë³„ ê°€ì¤‘ì¹˜ (ì´ 100%)
    # íŒŒì‹± 10, ì„ë² ë”© 25, ë§¤í•‘ 25, README ìƒì„± 35, ZIP 5
    update_progress(5, "í™˜ê²½ ì´ˆê¸°í™”â€¦")

    # 1) ì¹´í…Œê³ ë¦¬ íŒŒì‹± (10%)
    update_progress(10, "ğŸ“˜ ì¹´í…Œê³ ë¦¬ êµ¬ì¡° ë¶„ì„ ì¤‘â€¦")
    category_structure = load_category_structure(readme_file)

    # í´ë” ë¼ˆëŒ€ ìƒì„± (UI ë³€í™” ì—†ìŒ)
    folder_map = create_category_folders(output_dir, category_structure)
    update_progress(15, "ğŸ“‚ í´ë” êµ¬ì¡° ì¤€ë¹„ ì™„ë£Œ")

    # 2) ì„ë² ë”© (25%)
    update_progress(20, "ğŸ§  ë¸”ë¡œê·¸ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘â€¦")
    embeddings = prepare_blog_embeddings(blog_files)
    update_progress(35, "ğŸ§  ì„ë² ë”© ì™„ë£Œ")

    # 3) ë§¤í•‘ (25%)
    update_progress(40, "ğŸ“¦ ë¬¸ì„œë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë§¤í•‘ ì¤‘â€¦")
    mapping = match_documents_to_categories(embeddings, category_structure)
    update_progress(65, "ğŸ“¦ ë§¤í•‘ ì™„ë£Œ")

    # 4) README ìƒì„± (35%) â€” í•˜ìœ„ ë‹¨ìœ„ë³„ë¡œ ì„¸ë°€ ì§„í–‰ë¥ 
    # ì „ì²´ README ìƒì„± ê°œìˆ˜ ê³„ì‚°
    total_subtopics = sum(len(v.get("subtopics", [])) for v in category_structure)
    # ì‹¤ì œ ë¬¸ì„œê°€ ë§¤í•‘ëœ subtopicë§Œ ì§‘ê³„
    total_work_units = max(
        1,
        sum(len(files) > 0 for _, subtopics in mapping.items() for _, files in subtopics.items())
    )

    unit_weight = 35 / total_work_units  # ê°ê°ì˜ ì£¼ì œ ì™„ë£Œ ì‹œ ì§„í–‰ë¥  ë°˜ì˜
    cur_pct = 65
    update_progress(cur_pct, "ğŸ“ README ìš”ì•½ ìƒì„± ì‹œì‘â€¦")

    for category, subtopics in mapping.items():
        cat_folder = output_dir / sanitize_folder_name(category)
        cat_folder.mkdir(exist_ok=True)

        for sub, files in subtopics.items():
            if not files:
                continue

            sub_folder = cat_folder / sanitize_folder_name(sub)
            sub_folder.mkdir(exist_ok=True)

            # íŒŒì¼ ì €ì¥
            for f in files:
                (sub_folder / f.name).write_bytes(f.getvalue())

            # README ìƒì„±
            summary = generate_summary_readme(category, sub, files)
            (sub_folder / f"README_{sanitize_folder_name(sub)}.md").write_text(
                summary, encoding="utf-8"
            )

            # ì§„í–‰ë¥  ê°±ì‹ 
            cur_pct = min(100, int(cur_pct + unit_weight))
            update_progress(cur_pct, f"ğŸ“ README ìƒì„± ì¤‘â€¦ ({category} > {sub})")

    # 5) ZIP (5%)
    update_progress(95, "ğŸ“¦ ZIP íŒŒì¼ ìƒì„± ì¤‘â€¦")
    zip_path = Path("result_documents.zip")
    with zipfile.ZipFile(zip_path, "w") as z:
        for root, _, files in os.walk(output_dir):
            for f in files:
                p = os.path.join(root, f)
                z.write(p, arcname=os.path.relpath(p, output_dir))

    zip_placeholder.download_button(
        "[ Download Result ]",
        open("result_documents.zip", "rb"),
        file_name="categorized_blogs.zip",
        mime="application/zip",
        use_container_width=True,
    )

    update_progress(100, "âœ… ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ë° README ìš”ì•½ ì™„ë£Œ!")

# ê¸°ëŠ¥ ì˜ì—­ ----------------------------------------------------------------------------------------------------------------------------------------------------

else:
    progress_placeholder.progress(0)
    progress_text.markdown("<div class='status-bar'>[0%]</div>", unsafe_allow_html=True)
    log_box.markdown("<div class='log-box'>......</div>", unsafe_allow_html=True)
