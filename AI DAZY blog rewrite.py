# AI DAZY testmode
import streamlit as st
import zipfile
import os
import openai
import json
import hashlib
import re
import shutil
import secrets
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from pathlib import Path
from hdbscan import HDBSCAN


# ============================
# ğŸ”§ Recursive Split Settings
# ============================
MAX_FILES_PER_CLUSTER = 25
MAX_RECURSION_DEPTH = 2

# ============================
# ğŸ” Token Store (Server Memory)
# ============================
TOKEN_STORE = {}
TOKEN_EXPIRE_HOURS = 3

# ----------------------------
# ğŸŒˆ ê¸°ë³¸ í˜ì´ì§€ ì„¤ì •
# ----------------------------
st.set_page_config(
    page_title="AI dazy test mode",
    page_icon="ğŸ—‚ï¸",
    layout="wide",
)

# ============================
# ğŸ”’ Password + Token Gate
# ============================
APP_PASSWORD = st.secrets.get("APP_PASSWORD") or os.getenv("APP_PASSWORD")

params = st.experimental_get_query_params()
token = params.get("auth", [None])[0]

if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

# í† í° ìˆìœ¼ë©´ ì¸ì¦ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
if token:
    st.session_state.authenticated = True

# ë¹„ì¸ì¦ ìƒíƒœ â†’ ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
if not st.session_state.authenticated:
    st.markdown("<br><br><br><br>", unsafe_allow_html=True)
    col = st.columns([1, 2, 1])[1]

    with col:
        st.markdown(
            """
            <div style="
                background:var(--secondary-background-color);
                padding:2rem;
                border-radius:16px;
                text-align:center;
                color:var(--text-color);">
                <h2>ğŸ”’ Access Password</h2>
                <p>ì´ ì•±ì€ ì œí•œëœ ì‚¬ìš©ìë§Œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
            </div>
            """,
            unsafe_allow_html=True,
        )

        pw = st.text_input("Password", type="password", label_visibility="collapsed")

        if pw:
            if pw == APP_PASSWORD:
                new_token = secrets.token_hex(16)
                st.experimental_set_query_params(auth=new_token)
                st.session_state.authenticated = True
                st.success("ì ‘ê·¼ í—ˆìš©")
                st.rerun()
            else:
                st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    st.stop()

# ============================
# ğŸ”‘ API Key Input (First Time)
# ============================
if "api_key" not in st.session_state:
    st.markdown("### ğŸ”‘ OpenAI API Key")
 
    api_key_input = st.text_input(
        "OpenAI API Key",
        type="password",
        placeholder="sk-xxxxxxxxxxxxxxxxxxxxxxxx",
        label_visibility="collapsed",
    )
    st.caption("1ï¸âƒ£ í•´ë‹¹ì•±ì€ chat gpt / openaië¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ")
    st.caption("2ï¸âƒ£ openai ì—ì„œ ë°œê¸‰í•œ api key ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
    st.caption("3ï¸âƒ£ api key ë°œê¸‰ ë°›ê¸° : [ https://openai.com/ko-KR/api/ ]")
    
    if api_key_input:
        try:
            openai.api_key = api_key_input
            openai.Model.list()  # ìœ íš¨ì„± ê²€ì‚¬

            TOKEN_STORE[token] = {
                "api_key": api_key_input,
                "expires_at": datetime.utcnow() + timedelta(hours=TOKEN_EXPIRE_HOURS),
            }

            st.session_state.api_key = api_key_input
            st.success("API Key ì¸ì¦ ì™„ë£Œ")
            st.rerun()
        except Exception:
            st.error("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤.")

    st.stop()

# ============================
# ğŸ“ File Uploader State (ì´ˆê¸° 1íšŒ)
# ============================
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = 0

# ============================
# ğŸ¨ ìŠ¤íƒ€ì¼
# ============================
st.markdown(
"""
<style>

/* =========================
   ì•± ê¸°ë³¸ ë°°ê²½
========================= */
body {
    background-color: var(--background-color);
    font-family: 'Pretendard', sans-serif;
}

/* =========================
   ë²„íŠ¼ ìŠ¤íƒ€ì¼
========================= */
.stButton>button {
    border-radius: 10px;
    background-color: var(--primary-color);
    color: var(--text-color);

    /* ë°ì€ ë°°ê²½ì—ì„œ ê°€ë…ì„± í™•ë³´ */
    text-shadow: 0 1px 1px rgba(0,0,0,0.15);
    
    border: none;
    padding: 0.6em 1.2em;
    font-weight: 600;

    /* ë²„íŠ¼ ì „ìš© ê·¸ë¦¼ì */
    box-shadow: 0 4px 10px rgba(0, 0, 0, 0.18);

    transition:
        transform 0.15s ease,
        box-shadow 0.15s ease,
        filter 0.15s ease;
}

.stButton>button:hover {
    transform: translateY(-1px);
    box-shadow: 0 6px 14px rgba(0, 0, 0, 0.22);
    filter: brightness(0.97);
}

.stButton>button:active {
    transform: translateY(0);
    box-shadow: 0 3px 6px rgba(0, 0, 0, 0.25);
}

/* =========================
   ìƒíƒœë°”
========================= */
.status-bar {
    background-color: var(--secondary-background-color);
    color: var(--text-color);
    border-radius: 6px;
    padding: 0.5em;
    margin-top: 10px;
    font-size: 0.9em;

    /* ë²„íŠ¼ì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ëŠ” ìš”ì†Œ ì œê±° */
    box-shadow: none;
    border: none;
}

/* =========================
   ë¡œê·¸ ë°•ìŠ¤ (ì¹´ë“œ ìœ ì§€)
========================= */
.log-box {
    background-color: #dbede6;
    color: #050505;
    padding: 0.8em;
    margin-top: 10px;
    height: 120px;
    overflow-y: auto;
    font-size: 0.85em;

    /* ë°˜ì‘í˜• */
    border-radius: 12px;

    /* âŒ border ì œê±° */
    border: none;

    /* âœ… Streamlit ëŒ€ì‘ ìœ¤ê³½ */
    outline-offset: -1px;
    box-shadow: none;
}

/* =========================
   í…Œë§ˆ ë¯¸ì„¸ ì¡°ì •(ìƒíƒœë°” ì œì™¸)
========================= */
@media (prefers-color-scheme: dark) {
    .log-box {
        outline: 1.5px solid rgba(255, 255, 255, 0.16);
    }
}

@media (prefers-color-scheme: light) {
    .log-box {
        outline: 1.5px solid rgba(0, 0, 0, 0.28);
    }
}

</style>
""",
unsafe_allow_html=True,
)

# ============================
# ì‚¬ì´ë“œë°” ì„¤ì • ë¶€ë¶„
# ============================

# ----------------------------
# ìºì‹œ
# ----------------------------
CACHE_DIR = Path(".cache")
CACHE_DIR.mkdir(exist_ok=True)

def load_cache(p):
    try:
        return json.loads(p.read_text(encoding="utf-8")) if p.exists() else {}
    except Exception:
        return {}

def save_cache(p, d):
    p.write_text(json.dumps(d, ensure_ascii=False, indent=2), encoding="utf-8")

EMBED_CACHE = CACHE_DIR / "embeddings.json"
GROUP_CACHE = CACHE_DIR / "group_names.json"
README_CACHE = CACHE_DIR / "readmes.json"
EXPAND_CACHE = CACHE_DIR / "expands.json"

embedding_cache = load_cache(EMBED_CACHE)
group_cache = load_cache(GROUP_CACHE)
readme_cache = load_cache(README_CACHE)
expand_cache = load_cache(EXPAND_CACHE)

def reset_cache():
    if CACHE_DIR.exists():
        shutil.rmtree(CACHE_DIR)
    CACHE_DIR.mkdir(exist_ok=True)
    embedding_cache.clear()
    group_cache.clear()
    readme_cache.clear()
    expand_cache.clear()

def reset_output():
    output_dir = Path("output_docs")
    zip_path = Path("result_documents.zip")

    if output_dir.exists():
        shutil.rmtree(output_dir)
    if zip_path.exists():
        zip_path.unlink()

st.sidebar.markdown(
    """

"""
)

# ============================
#  ì‚¬ì´ë“œë°” UI
# ============================

# ----------------------------
# âœ… API Session Active (Sidebar)
# ----------------------------
openai.api_key = st.session_state.api_key

with st.sidebar:
    st.success("API ì¸ì¦ ì„±ê³µ")

# ----------------------------
# ğŸ”’ Logout Button
# ----------------------------
st.sidebar.title("âš™ï¸ Setting")
col1, col2 = st.sidebar.columns([1, 1], gap="small")

with col1:
    if st.button("API Key ë³€ê²½", use_container_width=True):
        st.session_state.pop("api_key", None)
        st.rerun()

with col2:
    if st.button("ë¡œê·¸ì•„ì›ƒ", use_container_width=True):
    # ì¸ì¦ ìƒíƒœ ì œê±°
        st.session_state.pop("authenticated", None)
        st.session_state.pop("api_key", None)

    # URL í† í° ì œê±°
        st.experimental_set_query_params()

    # ì „ì²´ ë¦¬ì…‹
        st.rerun()

st.sidebar.markdown("### ğŸ’¡ ì‚¬ìš© íŒ")
st.sidebar.markdown(
    """
- ğŸ“ íŒŒì¼ì„ **ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ ì‹œì‘** ë©ë‹ˆë‹¤.
- ğŸ“‚ **ì—¬ëŸ¬ ë¬¸ì„œë¥¼ í•œ ë²ˆì— ì—…ë¡œë“œ**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ğŸ§  ë¬¸ì„œëŠ” **AIê°€ í•˜ë‚˜ì˜ ë¸”ë¡œê·¸ ê¸€ë¡œ ë³‘í•©**í•©ë‹ˆë‹¤.
- âœï¸ SEO ì œëª© / ë©”íƒ€ / ë³¸ë¬¸ì„ ìë™ ìƒì„±í•©ë‹ˆë‹¤.
- â³ ë¬¸ì„œ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ì²˜ë¦¬ ì‹œê°„ì´ ëŠ˜ì–´ë‚©ë‹ˆë‹¤.
- ğŸ“¦ ì™„ë£Œ í›„ **ZIP íŒŒì¼ë¡œ í•œ ë²ˆì— ë‹¤ìš´ë¡œë“œ**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
)

# ============================
# ğŸ“ ë©”ì¸ UI
# ============================
left_col, right_col = st.columns([1, 1])

st.subheader("AI auto file analyzer")
st.caption("ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ìë™ìœ¼ë¡œ êµ¬ì¡°í™”í•©ë‹ˆë‹¤")

with left_col:
    st.subheader("File upload")
    uploaded_files = st.file_uploader(
        "ğŸ“ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (.md, .pdf, .txt)",
        accept_multiple_files=True,
        type=["md", "pdf", "txt"],
        key=f"uploader_{st.session_state.uploader_key}",
    )
    if st.button("Upload File Reset", use_container_width=True):
        st.session_state.uploader_key += 1
        st.rerun()
    # âœ… ë°˜ë“œì‹œ ì—¬ê¸° ì•ˆì—ì„œ
    col2, col3 = st.columns([1, 1], gap="small")

    with col2:
        if st.button("Cache Reset", use_container_width=True):
            reset_cache()
            st.rerun()
            
    with col3:
        if st.button("Download Reset", use_container_width=True):
            reset_output()
            st.rerun()


with right_col:
    st.subheader("ZIP Download")
    st.caption("ğŸ“ ë¬¸ì„œ ì •ë¦¬ í›„ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì´ í™œì„±í™” ë©ë‹ˆë‹¤.")

    zip_placeholder = st.empty()   # ğŸ‘ˆ ìœ„ì— ë‘ê³ 


# ============================
# âš™ï¸ ìƒíƒœ / ë¡œê·¸
# ============================
progress_placeholder = st.empty()
progress_text = st.empty()
log_box = st.empty()
logs = []

def log(msg):
    logs.append(msg)
    log_box.markdown(
        "<div class='log-box'>" + "<br>".join(logs[-10:]) + "</div>",
        unsafe_allow_html=True,
    )

def h(t: str):
    return hashlib.sha256(t.encode("utf-8")).hexdigest()


#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# =====================================================
# âœ¨ ìœ í‹¸
# =====================================================

def sanitize_folder_name(name: str) -> str:
    name = (name or "").strip()
    name = re.sub(r"[^\wê°€-í£\s\[\]]", "", name)
    name = re.sub(r"\s+", "_", name)
    return name.strip("_") or "ê¸°íƒ€_ë¬¸ì„œ"


def title_from_filename(file_name: str) -> str:
    base = file_name.rsplit(".", 1)[0]
    base = re.sub(r"[_\-]+", " ", base)
    return re.sub(r"\s+", " ", base).strip()


def build_readme_header(folder_path: str) -> str:
    return f"""<!--
README_ì†Œì†_í´ë”: {folder_path}
-->
"""


def readme_filename(folder_name: str, is_gap_report=False) -> str:
    if is_gap_report:
        return f"â˜…README_{folder_name}_ë³´ê°•_ë¦¬í¬íŠ¸.md"
    return f"â˜…README_{folder_name}.md"


# =====================================================
# ğŸ§  GPT EXPAND (ì¹´í…Œê³ ë¦¬ ê¸°ì¤€ ì˜ë¯¸ ë¶„ì„)
# =====================================================

def expand_document_with_gpt(file, category_readme_text):
    key = h(file.name + category_readme_text)
    if key in expand_cache:
        return expand_cache[key]

    fallback = title_from_filename(file.name)

    prompt = f"""
ë„ˆëŠ” ë¸”ë¡œê·¸ ì½˜í…ì¸  ë¶„ë¥˜ë¥¼ ìœ„í•œ ì˜ë¯¸ ë¶„ì„ê¸°ë‹¤.

[ë¸”ë¡œê·¸ ì¹´í…Œê³ ë¦¬ ë° ì„¸ë¶€ ì£¼ì œ ê¸°ì¤€]
{category_readme_text}

[ë¶„ì„ ëŒ€ìƒ ë¸”ë¡œê·¸ ì´ˆì•ˆ]
íŒŒì¼ëª…: {file.name}

ì¶œë ¥(JSON í•˜ë‚˜ë§Œ):
{{
  "canonical_title": "...",
  "parent_category": "ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ëª…",
  "sub_topic": "ì„¸ë¶€ ì£¼ì œëª…",
  "relation_reason": "ì£¼ì œì™€ì˜ ì—°ê´€ì„±",
  "synergy": "ê°™ì´ ë¬¶ì¼ ë•Œì˜ ì‹œë„ˆì§€",
  "goal_alignment": "ê³µí†µ ëª©í‘œ ë°©í–¥ì„±",
  "embedding_text": "..."
}}
"""

    try:
        r = openai.ChatCompletion.create(
            model="gpt-5-nano",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ë¸”ë¡œê·¸ ì½˜í…ì¸  ë¶„ì„ê¸°ë‹¤."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
        )
        data = json.loads(r["choices"][0]["message"]["content"])
    except Exception:
        data = {
            "canonical_title": fallback,
            "parent_category": "ê¸°íƒ€",
            "sub_topic": "ê¸°íƒ€",
            "relation_reason": "",
            "synergy": "",
            "goal_alignment": "",
            "embedding_text": fallback,
        }

    expand_cache[key] = data
    save_cache(EXPAND_CACHE, expand_cache)
    return data


def expand_documents_parallel(files, category_readme_text, max_workers=5):
    results = {}
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(expand_document_with_gpt, f, category_readme_text): f
            for f in files
        }
        for future in as_completed(futures):
            f = futures[future]
            results[f] = future.result()
    return results


# =====================================================
# ğŸ“„ README ìƒì„±
# =====================================================

def generate_topic_readme(category_title, topic, metas, folder_path):
    header = build_readme_header(folder_path)
    titles = [m["canonical_title"] for m in metas]

    prompt = f"""
ì¹´í…Œê³ ë¦¬ '{category_title}'ì˜ ì„¸ë¶€ ì£¼ì œ '{topic}'ì— ì†í•œ ê¸€ë“¤ì´ë‹¤.

READMEë¥¼ ì‘ì„±í•˜ë¼.

ë°˜ë“œì‹œ í¬í•¨:
- ì£¼ì œ ì„¤ëª…
- ê° ê¸€ê³¼ì˜ ì—°ê´€ì„±
- ê¸€ë“¤ ê°„ ì‹œë„ˆì§€
- ê³µí†µ ëª©í‘œ ë°©í–¥ì„±

ê¸€ ëª©ë¡:
{chr(10).join(titles)}
"""

    r = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” í•œêµ­ì–´ë¡œ READMEë¥¼ ì‘ì„±í•œë‹¤."},
            {"role": "user", "content": prompt},
        ],
    )

    return header + "\n" + r["choices"][0]["message"]["content"].strip()


# =====================================================
# ğŸ“„ ì¹´í…Œê³ ë¦¬ â†’ ê¸°ëŒ€ ì£¼ì œ ì¶”ì¶œ
# =====================================================

def extract_expected_topics(category_readme_text):
    prompt = f"""
ë‹¤ìŒ ë¬¸ì„œì—ì„œ ëŒ€ë¶„ë¥˜ì™€ ì„¸ë¶€ ì£¼ì œë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ì¶”ì¶œí•˜ë¼.
JSONë§Œ ì¶œë ¥í•˜ë¼.

í˜•ì‹:
{{ "ëŒ€ë¶„ë¥˜": ["ì„¸ë¶€ì£¼ì œ1", "ì„¸ë¶€ì£¼ì œ2"] }}

ë¬¸ì„œ:
{category_readme_text}
"""

    r = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ê¸°ë‹¤."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.1,
    )

    return json.loads(r["choices"][0]["message"]["content"])


def collect_actual_topics(expanded_docs):
    result = {}
    for meta in expanded_docs.values():
        result.setdefault(meta["parent_category"], set()).add(meta["sub_topic"])
    return {k: sorted(v) for k, v in result.items()}


def find_missing_topics(expected, actual):
    gaps = {}
    for parent, exp in expected.items():
        act = set(actual.get(parent, []))
        missing = [t for t in exp if t not in act]
        if missing:
            gaps[parent] = {
                "expected": exp,
                "actual": list(act),
                "missing": missing,
            }
    return gaps


def generate_gap_report_readme(category_title, gap_report, folder_path):
    header = build_readme_header(folder_path)

    prompt = f"""
ë¸”ë¡œê·¸ ì¹´í…Œê³ ë¦¬ '{category_title}'ì˜ ì½˜í…ì¸  ë³´ê°• ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ë¼.

í¬í•¨:
1. í˜„ì¬ êµ¬ì„± ìš”ì•½
2. ë¶€ì¡±í•œ ì„¸ë¶€ ì£¼ì œ
3. ì™œ ì¤‘ìš”í•œì§€
4. ë³´ê°• ì „ëµ

ë°ì´í„°:
{json.dumps(gap_report, ensure_ascii=False, indent=2)}
"""

    r = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ë¸”ë¡œê·¸ ì „ëµ ì»¨ì„¤í„´íŠ¸ë‹¤."},
            {"role": "user", "content": prompt},
        ],
    )

    return header + "\n" + r["choices"][0]["message"]["content"].strip()


# =====================================================
# ğŸ“ ì „ì²´ êµ¬ì¡° ìƒì„± (ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸)
# =====================================================

def build_structure(base_dir, category_title, category_readme_text, files):
    expanded = expand_documents_parallel(files, category_readme_text)

    root_name = f"í´ë”_{sanitize_folder_name(category_title)}"
    root_dir = os.path.join(base_dir, root_name)
    os.makedirs(root_dir, exist_ok=True)

    grouped = {}
    for file, meta in expanded.items():
        grouped.setdefault(
            (meta["parent_category"], meta["sub_topic"]),
            []
        ).append((file, meta))

    # ğŸ”¹ ì£¼ì œ í´ë” + README
    for (parent, topic), items in grouped.items():
        parent_name = f"í•˜ìœ„í´ë”_{sanitize_folder_name(parent)}"
        topic_name = f"ì£¼ì œ_{sanitize_folder_name(topic)}"

        parent_dir = os.path.join(root_dir, parent_name)
        topic_dir = os.path.join(parent_dir, topic_name)
        os.makedirs(topic_dir, exist_ok=True)

        metas = []
        for file, meta in items:
            os.rename(file.path, os.path.join(topic_dir, file.name))
            metas.append(meta)

        folder_path = f"{root_name} / {parent_name} / {topic_name}"
        readme = generate_topic_readme(
            category_title,
            topic,
            metas,
            folder_path
        )

        with open(
            os.path.join(topic_dir, readme_filename(topic_name)),
            "w",
            encoding="utf-8"
        ) as f:
            f.write(readme)

    # ğŸ”¹ ìµœìƒìœ„ README
    top_header = build_readme_header(root_name)
    with open(
        os.path.join(root_dir, readme_filename(root_name)),
        "w",
        encoding="utf-8"
    ) as f:
        f.write(top_header + f"\n# {category_title}\n")

    # ğŸ”¹ ë³´ê°• ë¦¬í¬íŠ¸
    expected = extract_expected_topics(category_readme_text)
    actual = collect_actual_topics(expanded)
    gaps = find_missing_topics(expected, actual)

    if gaps:
        gap_readme = generate_gap_report_readme(
            category_title,
            gaps,
            root_name
        )
        with open(
            os.path.join(
                root_dir,
                readme_filename(root_name, is_gap_report=True)
            ),
            "w",
            encoding="utf-8"
        ) as f:
            f.write(gap_readme)


#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ----------------------------
# ğŸš€ ë©”ì¸ ì²˜ë¦¬ (ì¹´í…Œê³ ë¦¬ ê¸°ì¤€ ë²„ì „)
# ----------------------------
if uploaded_files:
    uploaded_files = [f for f in uploaded_files if f and f.name.strip()]
    if not uploaded_files:
        st.stop()

    reset_output()

    output_dir = Path("output_docs")
    output_dir.mkdir(exist_ok=True)

    progress = progress_placeholder.progress(0)
    progress_text.markdown("<div class='status-bar'>[0%]</div>", unsafe_allow_html=True)

    log("[íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ]")

    # ğŸ”¹ ì¹´í…Œê³ ë¦¬ README / ì´ˆì•ˆ ë¶„ë¦¬
    category_file = None
    draft_files = []

    for f in uploaded_files:
        if "README" in f.name:
            category_file = f
        else:
            draft_files.append(f)

    if not category_file or not draft_files:
        st.error("ì¹´í…Œê³ ë¦¬ README 1ê°œì™€ ë¸”ë¡œê·¸ ì´ˆì•ˆ íŒŒì¼ë“¤ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        st.stop()

    category_text = category_file.getvalue().decode("utf-8")

    # ğŸ”¹ ì„ì‹œ íŒŒì¼ ê°ì²´ ìƒì„± (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜)
    class TempFile:
        def __init__(self, f):
            self.name = f.name
            self.path = output_dir / f.name
            self._data = f.getvalue()
            self.path.write_bytes(self._data)

    temp_files = [TempFile(f) for f in draft_files]

    total = len(temp_files)
    done = 0
    pct = 30

    progress.progress(pct)
    progress_text.markdown(
        f"<div class='status-bar'>| ì¹´í…Œê³ ë¦¬ ë¶„ì„ì¤‘â€¦ | [ {pct}%  ({done} / {total} file) ]</div>",
        unsafe_allow_html=True
    )

    # âœ… ì¶”ê°€: ì‹¤ì œ ì§„í–‰ë¥  ì½œë°± (ì´ê²ƒë§Œ ì¶”ê°€ë¨)
    def progress_cb(done, total, phase):
        pct = int(done / total * 100) if total else 100
        progress.progress(pct)
        progress_text.markdown(
            f"<div class='status-bar'>| {phase} | [ {pct}%  ({done} / {total} file) ]</div>",
            unsafe_allow_html=True
        )

        build_structure(
            base_dir=output_dir,
            category_title=category_file.name.rsplit(".", 1)[0],
            category_readme_text=category_text,
            files=temp_files,
            progress_cb=progress_cb,   # âœ… ì¶”ê°€
        )

    done = total
    pct = 80
    progress.progress(pct)
    progress_text.markdown(
        f"<div class='status-bar'>| ì •ë¦¬ ì¤‘â€¦ | [ {pct}%  ({done} / {total} file) ]</div>",
        unsafe_allow_html=True
    )

    # ğŸ”¹ ZIP ìƒì„±
    zip_path = Path("result_documents.zip")
    with zipfile.ZipFile(zip_path, "w") as z:
        for root, _, files in os.walk(output_dir):
            for f in files:
                p = os.path.join(root, f)
                z.write(p, arcname=os.path.relpath(p, output_dir))

    zip_placeholder.download_button(
        "[ Download ]",
        open(zip_path, "rb"),
        file_name="result_documents.zip",
        mime="application/zip",
        use_container_width=True,
        key="zip_download",
    )

    progress.progress(100)
    progress_text.markdown("<div class='status-bar'>[100% complete]</div>", unsafe_allow_html=True)
    log("ëª¨ë“  ë¬¸ì„œ ì •ë¦¬ ì™„ë£Œ")



else:
    progress_placeholder.progress(0)
    progress_text.markdown("<div class='status-bar'>[0%]</div>", unsafe_allow_html=True)
    log_box.markdown("<div class='log-box'>......</div>", unsafe_allow_html=True)
