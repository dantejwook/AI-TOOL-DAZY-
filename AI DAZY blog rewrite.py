# AI DAZY TEST MODE

# ê¸°ë³¸ ì˜ì—­ ----------------------------------------------------------------------------------------------------------------------------------------------------

import streamlit as st
import zipfile
import os
import openai
import json
import hashlib
import re
import shutil
import secrets
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from pathlib import Path
from hdbscan import HDBSCAN


# ============================
# ğŸ”§ Recursive Split Settings
# ============================
MAX_FILES_PER_CLUSTER = 25
MAX_RECURSION_DEPTH = 2

# ============================
# ğŸ” Token Store (Server Memory)
# ============================
TOKEN_STORE = {}
TOKEN_EXPIRE_HOURS = 3

# ------------------------------------------
# ğŸŒˆ ê¸°ë³¸ í˜ì´ì§€ ì„¤ì •
# ------------------------------------------
st.set_page_config(
    page_title="AI dazy test mode",
    page_icon="ğŸ—‚ï¸",
    layout="wide",
)
# ============================
# ğŸ”’ Password + Token Gate
# ============================
APP_PASSWORD = st.secrets.get("APP_PASSWORD") or os.getenv("APP_PASSWORD")

params = st.experimental_get_query_params()
token = params.get("auth", [None])[0]

if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

# í† í° ìˆìœ¼ë©´ ì¸ì¦ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
if token:
    st.session_state.authenticated = True

# ë¹„ì¸ì¦ ìƒíƒœ â†’ ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
if not st.session_state.authenticated:
    st.markdown("<br><br><br><br>", unsafe_allow_html=True)
    col = st.columns([1, 2, 1])[1]

    with col:
        st.markdown(
            """
            <div style="
                background:var(--secondary-background-color);
                padding:2rem;
                border-radius:16px;
                text-align:center;
                color:var(--text-color);">
                <h2>ğŸ”’ Access Password</h2>
                <p>ì´ ì•±ì€ ì œí•œëœ ì‚¬ìš©ìë§Œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
            </div>
            """,
            unsafe_allow_html=True,
        )

        pw = st.text_input("Password", type="password", label_visibility="collapsed")

        if pw:
            if pw == APP_PASSWORD:
                new_token = secrets.token_hex(16)
                st.experimental_set_query_params(auth=new_token)
                st.session_state.authenticated = True
                st.success("ì ‘ê·¼ í—ˆìš©")
                st.rerun()
            else:
                st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    st.stop()

# ============================
# ğŸ”‘ API Key Input (First Time)
# ============================
if "api_key" not in st.session_state:
    st.markdown("### ğŸ”‘ OpenAI API Key")
 
    api_key_input = st.text_input(
        "OpenAI API Key",
        type="password",
        placeholder="sk-xxxxxxxxxxxxxxxxxxxxxxxx",
        label_visibility="collapsed",
    )
    st.caption("1ï¸âƒ£ í•´ë‹¹ì•±ì€ chat gpt / openaië¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ")
    st.caption("2ï¸âƒ£ openai ì—ì„œ ë°œê¸‰í•œ api key ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
    st.caption("3ï¸âƒ£ api key ë°œê¸‰ ë°›ê¸° : [ https://openai.com/ko-KR/api/ ]")
    
    if api_key_input:
        try:
            openai.api_key = api_key_input
            openai.Model.list()  # ìœ íš¨ì„± ê²€ì‚¬

            TOKEN_STORE[token] = {
                "api_key": api_key_input,
                "expires_at": datetime.utcnow() + timedelta(hours=TOKEN_EXPIRE_HOURS),
            }

            st.session_state.api_key = api_key_input
            st.success("API Key ì¸ì¦ ì™„ë£Œ")
            st.rerun()
        except Exception:
            st.error("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤.")

    st.stop()

# ============================
# ğŸ“ File Uploader State (ì´ˆê¸° 1íšŒ)
# ============================
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = 0

# ============================
# ğŸ¨ ìŠ¤íƒ€ì¼
# ============================
st.markdown(
"""
<style>

/* =========================
   ì•± ê¸°ë³¸ ë°°ê²½
========================= */
body {
    background-color: var(--background-color);
    font-family: 'Pretendard', sans-serif;
}

/* =========================
   ë²„íŠ¼ ìŠ¤íƒ€ì¼
========================= */
.stButton>button {
    border-radius: 10px;
    background-color: var(--primary-color);
    color: var(--text-color);

    /* ë°ì€ ë°°ê²½ì—ì„œ ê°€ë…ì„± í™•ë³´ */
    text-shadow: 0 1px 1px rgba(0,0,0,0.15);
    
    border: none;
    padding: 0.6em 1.2em;
    font-weight: 600;

    /* ë²„íŠ¼ ì „ìš© ê·¸ë¦¼ì */
    box-shadow: 0 4px 10px rgba(0, 0, 0, 0.18);

    transition:
        transform 0.15s ease,
        box-shadow 0.15s ease,
        filter 0.15s ease;
}

.stButton>button:hover {
    transform: translateY(-1px);
    box-shadow: 0 6px 14px rgba(0, 0, 0, 0.22);
    filter: brightness(0.97);
}

.stButton>button:active {
    transform: translateY(0);
    box-shadow: 0 3px 6px rgba(0, 0, 0, 0.25);
}

/* =========================
   ìƒíƒœë°”
========================= */
.status-bar {
    background-color: var(--secondary-background-color);
    color: var(--text-color);
    border-radius: 6px;
    padding: 0.5em;
    margin-top: 10px;
    font-size: 0.9em;

    /* ë²„íŠ¼ì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ëŠ” ìš”ì†Œ ì œê±° */
    box-shadow: none;
    border: none;
}

/* =========================
   ë¡œê·¸ ë°•ìŠ¤ (ì¹´ë“œ ìœ ì§€)
========================= */
.log-box {
    background-color: #dbede6;
    color: #050505;
    padding: 0.8em;
    margin-top: 10px;
    height: 120px;
    overflow-y: auto;
    font-size: 0.85em;

    /* ë°˜ì‘í˜• */
    border-radius: 12px;

    /* âŒ border ì œê±° */
    border: none;

    /* âœ… Streamlit ëŒ€ì‘ ìœ¤ê³½ */
    outline-offset: -1px;
    box-shadow: none;
}

/* =========================
   í…Œë§ˆ ë¯¸ì„¸ ì¡°ì •(ìƒíƒœë°” ì œì™¸)
========================= */
@media (prefers-color-scheme: dark) {
    .log-box {
        outline: 1.5px solid rgba(255, 255, 255, 0.16);
    }
}

@media (prefers-color-scheme: light) {
    .log-box {
        outline: 1.5px solid rgba(0, 0, 0, 0.28);
    }
}

</style>
""",
unsafe_allow_html=True,
)

# ============================
# ì‚¬ì´ë“œë°” ì„¤ì • ë¶€ë¶„
# ============================

# ------------------------------------------
# ìºì‹œ
# ------------------------------------------
CACHE_DIR = Path(".cache")
CACHE_DIR.mkdir(exist_ok=True)

def load_cache(p):
    try:
        return json.loads(p.read_text(encoding="utf-8")) if p.exists() else {}
    except Exception:
        return {}

def save_cache(p, d):
    p.write_text(json.dumps(d, ensure_ascii=False, indent=2), encoding="utf-8")

EMBED_CACHE = CACHE_DIR / "embeddings.json"
GROUP_CACHE = CACHE_DIR / "group_names.json"
README_CACHE = CACHE_DIR / "readmes.json"
EXPAND_CACHE = CACHE_DIR / "expands.json"

embedding_cache = load_cache(EMBED_CACHE)
group_cache = load_cache(GROUP_CACHE)
readme_cache = load_cache(README_CACHE)
expand_cache = load_cache(EXPAND_CACHE)

def reset_cache():
    if CACHE_DIR.exists():
        shutil.rmtree(CACHE_DIR)
    CACHE_DIR.mkdir(exist_ok=True)
    embedding_cache.clear()
    group_cache.clear()
    readme_cache.clear()
    expand_cache.clear()

def reset_output():
    output_dir = Path("output_docs")
    zip_path = Path("result_documents.zip")

    if output_dir.exists():
        shutil.rmtree(output_dir)
    if zip_path.exists():
        zip_path.unlink()

st.sidebar.markdown(
    """

"""
)

# ============================
#  ì‚¬ì´ë“œë°” UI
# ============================

# ------------------------------------------
# âœ… API Session Active (Sidebar)
# ------------------------------------------
openai.api_key = st.session_state.api_key

with st.sidebar:
    st.success("API ì¸ì¦ ì„±ê³µ")

# ------------------------------------------
# ğŸ”’ Logout Button
# ------------------------------------------
st.sidebar.title("âš™ï¸ Setting")
col1, col2 = st.sidebar.columns([1, 1], gap="small")

with col1:
    if st.button("API Key ë³€ê²½", use_container_width=True):
        st.session_state.pop("api_key", None)
        st.rerun()

with col2:
    if st.button("ë¡œê·¸ì•„ì›ƒ", use_container_width=True):
    # ì¸ì¦ ìƒíƒœ ì œê±°
        st.session_state.pop("authenticated", None)
        st.session_state.pop("api_key", None)

    # URL í† í° ì œê±°
        st.experimental_set_query_params()

    # ì „ì²´ ë¦¬ì…‹
        st.rerun()

st.sidebar.markdown("### ğŸ’¡ ì‚¬ìš© íŒ")
st.sidebar.markdown(
    """
- ğŸ“ íŒŒì¼ì„ **ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ ì‹œì‘** ë©ë‹ˆë‹¤.
- ğŸ“‚ **ì—¬ëŸ¬ ë¬¸ì„œë¥¼ í•œ ë²ˆì— ì—…ë¡œë“œ**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ğŸ§  ë¬¸ì„œëŠ” **AIê°€ ìë™ìœ¼ë¡œ ì£¼ì œë³„ ë¶„ë¥˜**í•©ë‹ˆë‹¤.
- ğŸ“ í´ë” ìˆ˜ê°€ ë§ìœ¼ë©´ **ìë™ìœ¼ë¡œ í•˜ìœ„ í´ë”ë¡œ ë¶„í•´**ë©ë‹ˆë‹¤.
- â³ ë¬¸ì„œ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ì²˜ë¦¬ ì‹œê°„ì´ ëŠ˜ì–´ë‚©ë‹ˆë‹¤.
- ğŸ“¦ ì™„ë£Œ í›„ **ZIP íŒŒì¼ë¡œ í•œ ë²ˆì— ë‹¤ìš´ë¡œë“œ**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
)

# ============================
# ğŸ“ ë©”ì¸ UI
# ============================

left_col, right_col = st.columns([1, 1])

if st.button("ğŸš€ ì‹¤í–‰", use_container_width=True):
    if not api_key or not readme_file or not content_files:
        st.warning("README íŒŒì¼, ì´ˆì•ˆ íŒŒì¼ì„ ëª¨ë‘ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        with st.spinner("AIê°€ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œìš”!"):
            result_zip = process_documents(readme_file, content_files, api_key)
            st.success("âœ… ì²˜ë¦¬ ì™„ë£Œ! ì•„ë˜ì—ì„œ ZIPì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
            st.download_button("ğŸ“¦ ê²°ê³¼ ZIP ë‹¤ìš´ë¡œë“œ", open(result_zip, "rb"), file_name="AI_Blog_Sorted.zip")

st.subheader("AI auto file analyzer")
st.caption("ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ìë™ìœ¼ë¡œ êµ¬ì¡°í™”í•©ë‹ˆë‹¤")

with left_col:
    st.subheader("File upload")
    readme_file = st.file_uploader("ğŸ“˜ ë¸”ë¡œê·¸ ì¹´í…Œê³ ë¦¬ README íŒŒì¼ ì—…ë¡œë“œ", type=["md"])
    content_files = st.file_uploader("ğŸ“„ ë¸”ë¡œê·¸ ì´ˆì•ˆ íŒŒì¼ ì—…ë¡œë“œ (ë³µìˆ˜ ê°€ëŠ¥)"
    (
        "ğŸ“ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (.md, .pdf, .txt)",
        accept_multiple_files=True,
        type=["md", "pdf", "txt"],
        key=f"uploader_{st.session_state.uploader_key}",
    )
    if st.button("Upload File Reset", use_container_width=True):
        st.session_state.uploader_key += 1
        st.rerun()
    # âœ… ë°˜ë“œì‹œ ì—¬ê¸° ì•ˆì—ì„œ
    col2, col3 = st.columns([1, 1], gap="small")

    with col2:
        if st.button("Cache Reset", use_container_width=True):
            reset_cache()
            st.rerun()
            
    with col3:
        if st.button("Download Reset", use_container_width=True):
            reset_output()
            st.rerun()


with right_col:
    st.subheader("ZIP Download")
    st.caption("ğŸ“ ë¬¸ì„œ ì •ë¦¬ í›„ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì´ í™œì„±í™” ë©ë‹ˆë‹¤.")

    zip_placeholder = st.empty()   # ğŸ‘ˆ ìœ„ì— ë‘ê³ 


# ============================
# âš™ï¸ ìƒíƒœ / ë¡œê·¸
# ============================
progress_placeholder = st.empty()
progress_text = st.empty()
log_box = st.empty()
logs = []

def log(msg):
    logs.append(msg)
    log_box.markdown(
        "<div class='log-box'>" + "<br>".join(logs[-10:]) + "</div>",
        unsafe_allow_html=True,
    )

def h(t: str):
    return hashlib.sha256(t.encode("utf-8")).hexdigest()

# ê¸°ë³¸ ì˜ì—­ ----------------------------------------------------------------------------------------------------------------------------------------------------

# ê¸°ëŠ¥ ì˜ì—­ ----------------------------------------------------------------------------------------------------------------------------------------------------



# -------------------------------------------
# âœ¨ ìœ í‹¸ [ê²½ë¡œ, ìºì‹œ, íŒŒì¼ëª…, í•´ì‹œ ë“± ê³µí†µ í•¨ìˆ˜]
# -------------------------------------------

def h(text): 
    return sha256(text.encode("utf-8")).hexdigest()

def sanitize_folder_name(name: str) -> str:
    name = (name or "").strip()
    name = re.sub(r"[^\wê°€-í£\s\-\_]", "", name)
    name = re.sub(r"\s+", "_", name)
    return name.strip("_") or "ê¸°íƒ€_ë¬¸ì„œ"

def unique_folder_name(base: str, existing: set) -> str:
    if base not in existing:
        return base
    i = 1
    while f"{base}_{i}" in existing:
        i += 1
    return f"{base}_{i}"

def save_text(path, content):
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(content, encoding="utf-8")

def load_text(file):
    if file.name.endswith(".pdf"):
        import fitz
        text = ""
        with fitz.open(stream=file.read(), filetype="pdf") as doc:
            for page in doc:
                text += page.get_text("text")
        return text
    else:
        return file.read().decode("utf-8", errors="ignore")


# ------------------------------------------
# ğŸ“˜ ì¹´í…Œê³ ë¦¬ README ê¸°ë°˜ í´ë” ìƒì„±
# ------------------------------------------

def parse_readme_structure(readme_text: str) -> dict:
    structure = {}
    current_main, current_sub = None, None
    for line in readme_text.splitlines():
        line = line.strip()
        if line.startswith("# "):
            current_main = line[2:].strip()
            structure[current_main] = {}
        elif line.startswith("## "):
            current_sub = line[3:].strip()
            structure[current_main][current_sub] = []
        elif line.startswith("### "):
            topic = line[4:].strip()
            structure[current_main].setdefault(current_sub, []).append(topic)
    return structure


# ---------------------------------------------------
# ğŸ§  0ì°¨ GPT EXPAND [ê° ë¬¸ì„œë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ì •ê·œí™”í•˜ëŠ” ë‹¨ê³„]
# ---------------------------------------------------

def expand_document_with_gpt(file, api_key):
    openai.api_key = api_key
    content = load_text(file)
    prompt = f"""
    ì•„ë˜ ë¬¸ì„œëŠ” ë¸”ë¡œê·¸ ì´ˆì•ˆì…ë‹ˆë‹¤.
    ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ 3~4ì¤„ë¡œ ìš”ì•½í•˜ê³ , ì˜ë¯¸ ê¸°ë°˜ ë²¡í„°í™”ë¥¼ ìœ„í•œ ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
    ì¶œë ¥ í˜•ì‹(JSON):
    {{
      "title": "ë¬¸ì„œì˜ ëŒ€í‘œ ì œëª©",
      "summary": "ë¬¸ì„œì˜ í•µì‹¬ ìš”ì•½",
      "embedding_text": "ì˜ë¯¸ ê¸°ë°˜ ë²¡í„°í™”ë¥¼ ìœ„í•œ í™•ì¥ í…ìŠ¤íŠ¸"
    }}
    """
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ë¸”ë¡œê·¸ ë¬¸ì„œ ì˜ë¯¸ ë¶„ì„ ì „ë¬¸ê°€ë‹¤."},
                {"role": "user", "content": prompt + "\n\n" + content[:2500]}
            ],
            temperature=0.2,
        )
        data = json.loads(response["choices"][0]["message"]["content"])
        return data
    except Exception as e:
        return {"title": file.name, "summary": "ìš”ì•½ ì‹¤íŒ¨", "embedding_text": file.name}


# ---------------------------------------------------
# â­ ì¶”ê°€: 0ì°¨ EXPAND ë³‘ë ¬ ì²˜ë¦¬[ìœ„ ë‹¨ê³„ì˜ ë³‘ë ¬í™” ë²„ì „]
# ---------------------------------------------------

def expand_documents_parallel(files, api_key):
    results = []
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = [executor.submit(expand_document_with_gpt, f, api_key) for f in files]
        for future in as_completed(futures):
            results.append(future.result())
    return results


# ------------------------------------------
# âœ¨ ì„ë² ë”© [ë²¡í„°í™”]
# ------------------------------------------

def embed_texts(texts, api_key):
    openai.api_key = api_key
    try:
        res = openai.Embedding.create(model="text-embedding-3-large", input=texts)
        return [d["embedding"] for d in res["data"]]
    except Exception as e:
        print("ì„ë² ë”© ì‹¤íŒ¨:", e)
        return [[0.0]*1536 for _ in texts]


# ------------------------------------------
# ğŸ“¦ í´ëŸ¬ìŠ¤í„°ë§ [ìœ ì‚¬ ë¬¸ì„œ ë¬¶ê¸°]
# ------------------------------------------

def cluster_documents(embeddings):
    clusterer = HDBSCAN(min_cluster_size=2, min_samples=1)
    return clusterer.fit_predict(embeddings)


# ------------------------------------------
# ğŸ” ìë™ ì¬ë¶„í•´ [ëŒ€í˜• í´ëŸ¬ìŠ¤í„°ë¥¼ ë‹¤ì‹œ ì„¸ë¶„í™”]
# ------------------------------------------

def recursive_cluster(files, embeddings, depth=0, max_depth=2):
    if len(files) <= 25 or depth >= max_depth:
        return [files]
    labels = cluster_documents(embeddings)
    groups = {}
    for f, l in zip(files, labels):
        groups.setdefault(l, []).append(f)
    result = []
    for g in groups.values():
        if len(g) > 25:
            sub_embeddings = [embeddings[i] for i, f in enumerate(files) if f in g]
            result.extend(recursive_cluster(g, sub_embeddings, depth+1))
        else:
            result.append(g)
    return result


# ----------------------------------------------------
# âœ¨ GPT í´ë”ëª… / README [ê° ê·¸ë£¹ ì´ë¦„ ê²°ì • + README ìƒì„±]
# ----------------------------------------------------

def generate_group_name(docs, api_key):
    openai.api_key = api_key
    titles = "\n".join([d["title"] for d in docs])
    prompt = f"""
    ë‹¤ìŒ ê¸€ ì œëª© ëª©ë¡ì„ ë³´ê³ , ì´ ê·¸ë£¹ì˜ ê³µí†µ ì£¼ì œëª…ì„ 1ì¤„ë¡œ ì •ë¦¬í•˜ì„¸ìš”.
    ì˜ˆì‹œ: "êµ­ë‚´ ë·°í‹°ì—…ê³„ íŠ¸ë Œë“œ ë³€í™”", "ì •ì±… ë° ì‹œì¥ ì´ìŠˆ"
    ì¶œë ¥: ê³µí†µ ì£¼ì œ í•œ ì¤„ë§Œ.
    """
    response = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt + "\n\n" + titles}]
    )
    return sanitize_folder_name(response["choices"][0]["message"]["content"].strip())


def generate_readme(group_name, docs, api_key):
    openai.api_key = api_key
    doc_summaries = "\n".join([f"- {d['title']}: {d['summary']}" for d in docs])
    prompt = f"""
    '{group_name}' í´ë”ì˜ READMEë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    ì´ í´ë”ì˜ ê¸€ë“¤ì´ ì–´ë–¤ ì—°ê´€ì„±ê³¼ ì‹œë„ˆì§€ë¥¼ ê°€ì§€ë©°, ê³µí†µ ëª©í‘œê°€ ë¬´ì—‡ì¸ì§€ 3~5ë¬¸ë‹¨ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
    ë¬¸ì„œ ëª©ë¡:
    {doc_summaries}
    """
    response = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )
    return response["choices"][0]["message"]["content"]


# ------------------------------------------
# ğŸš€ ë©”ì¸ ì²˜ë¦¬ [í”„ë¡œê·¸ë¨ ì§„í–‰]
# ------------------------------------------

def process_documents(readme_file, content_files, api_key):
    # 1ï¸âƒ£ README êµ¬ì¡° íŒŒì‹±
    structure = parse_readme_structure(load_text(readme_file))
    base = Path("output_docs")
    if base.exists(): shutil.rmtree(base)
    base.mkdir(exist_ok=True)
    for cat, subs in structure.items():
        for sub in subs.keys():
            (base / sanitize_folder_name(cat) / sanitize_folder_name(sub)).mkdir(parents=True, exist_ok=True)

    # 2ï¸âƒ£ ë¬¸ì„œ ì˜ë¯¸ í™•ì¥ + ì„ë² ë”©
    expanded = expand_documents_parallel(content_files, api_key)
    embeddings = embed_texts([e["embedding_text"] for e in expanded], api_key)

    # 3ï¸âƒ£ í´ëŸ¬ìŠ¤í„°ë§ ë° ë¶„ë¥˜
    labels = cluster_documents(embeddings)
    clusters = {}
    for f, l in zip(expanded, labels):
        clusters.setdefault(l, []).append(f)

    # 4ï¸âƒ£ í´ë” ìƒì„± ë° README ì‘ì„±
    for cluster_id, docs in clusters.items():
        group_name = generate_group_name(docs, api_key)
        group_path = base / group_name
        group_path.mkdir(exist_ok=True)
        readme_text = generate_readme(group_name, docs, api_key)
        save_text(group_path / f"README_{group_name}.md", readme_text)

        for d in docs:
            save_text(group_path / f"{sanitize_folder_name(d['title'])}.txt", d['summary'])

    # 5ï¸âƒ£ ZIP íŒŒì¼ë¡œ ë¬¶ê¸°
    zip_path = Path("result.zip")
    with zipfile.ZipFile(zip_path, "w") as z:
        for root, _, files in os.walk(base):
            for f in files:
                path = Path(root) / f
                z.write(path, arcname=path.relative_to(base))
    return zip_path

# ê¸°ëŠ¥ ì˜ì—­ ----------------------------------------------------------------------------------------------------------------------------------------------------

else:
    progress_placeholder.progress(0)
    progress_text.markdown("<div class='status-bar'>[0%]</div>", unsafe_allow_html=True)
    log_box.markdown("<div class='log-box'>......</div>", unsafe_allow_html=True)
