import streamlit as st
import zipfile
import os
from pathlib import Path
import openai
from hdbscan import HDBSCAN
import json
import hashlib
import re

# ----------------------------
# ğŸŒˆ ê¸°ë³¸ í˜ì´ì§€ ì„¤ì •
# ----------------------------
st.set_page_config(page_title="AI dazy document sorter", page_icon="ğŸ—‚ï¸", layout="wide")

# ----------------------------
# ğŸ” OpenAI API í‚¤ ì„¤ì • (legacy)
# ----------------------------
openai.api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")

if not openai.api_key:
    st.sidebar.error("ğŸš¨ OpenAI API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()
else:
    st.sidebar.success("âœ… OpenAI Key ë¡œë“œ ì™„ë£Œ")

# ----------------------------
# ğŸ¨ ìŠ¤íƒ€ì¼
# ----------------------------
st.markdown(
    """
    <style>
    body { background-color: #f8f9fc; font-family: 'Pretendard', sans-serif; }
    .status-bar { background:#595656; border-radius:6px; padding:0.5em; margin-top:10px; }
    .log-box { background:#595656; border-radius:6px; padding:0.8em; height:120px; overflow-y:auto; }
    </style>
    """,
    unsafe_allow_html=True,
)

# ----------------------------
# ğŸ§­ ì‚¬ì´ë“œë°”
# ----------------------------
st.sidebar.title("âš™ï¸ ì„¤ì •")
if st.sidebar.button("ğŸ” ë‹¤ì‹œ ì‹œì‘"):
    st.session_state.clear()
    st.rerun()

lang = st.sidebar.selectbox("ğŸŒ ì–¸ì–´ ì„ íƒ", ["í•œêµ­ì–´", "English"])

# ----------------------------
# ğŸ“ UI
# ----------------------------
left_col, right_col = st.columns([1, 1])

with left_col:
    st.subheader("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader(
        "ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (.md, .pdf, .txt)",
        accept_multiple_files=True,
        type=["md", "pdf", "txt"],
    )

with right_col:
    st.subheader("ğŸ“¦ ZIP ë‹¤ìš´ë¡œë“œ")
    zip_placeholder = st.empty()

# ----------------------------
# ğŸ“Š ìƒíƒœ / ë¡œê·¸
# ----------------------------
progress_placeholder = st.empty()
progress_text = st.empty()
log_box = st.empty()
logs = []

def log(msg):
    logs.append(msg)
    log_box.markdown(
        "<div class='log-box'>" + "<br>".join(logs[-10:]) + "</div>",
        unsafe_allow_html=True,
    )

# ----------------------------
# ğŸ§  ìºì‹œ
# ----------------------------
CACHE = Path(".cache")
CACHE.mkdir(exist_ok=True)

def h(t): return hashlib.sha256(t.encode()).hexdigest()

def load(p): return json.loads(p.read_text()) if p.exists() else {}
def save(p, d): p.write_text(json.dumps(d, ensure_ascii=False, indent=2))

emb_cache = load(CACHE / "emb.json")
grp_cache = load(CACHE / "grp.json")
sub_cache = load(CACHE / "sub.json")
readme_cache = load(CACHE / "readme.json")

# ----------------------------
# ğŸ”§ ìœ í‹¸
# ----------------------------
def sanitize(name):
    name = re.sub(r"[^a-z0-9]+", "_", name.lower())
    name = re.sub(r"_+", "_", name).strip("_")
    return name if re.search(r"[a-z]", name) else "misc_documents"

# ----------------------------
# ğŸ¤– OpenAI í•¨ìˆ˜
# ----------------------------
def embed_titles(titles):
    vectors = []
    missing = []

    for t in titles:
        k = h(t)
        if k in emb_cache:
            vectors.append(emb_cache[k])
        else:
            missing.append(t)

    if missing:
        r = openai.Embedding.create(
            model="text-embedding-3-large",
            input=missing,
        )
        for t, d in zip(missing, r["data"]):
            emb_cache[h(t)] = d["embedding"]
        save(CACHE / "emb.json", emb_cache)

        vectors = [emb_cache[h(t)] for t in titles]

    return vectors

def generate_group_name(names):
    k = h("||".join(sorted(names)))
    if k in grp_cache:
        return grp_cache[k]

    r = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": "\n".join(names)}],
        temperature=0.2,
    )
    name = sanitize(r["choices"][0]["message"]["content"])
    grp_cache[k] = name
    save(CACHE / "grp.json", grp_cache)
    return name

def generate_subgroups(names):
    k = h("SUB||" + "||".join(sorted(names)))
    if k in sub_cache:
        return sub_cache[k]

    prompt = f"""
ë‹¤ìŒ ë¬¸ì„œ ì œëª©ë“¤ì„ 2~4ê°œì˜ í•˜ìœ„ ì£¼ì œë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
ê²°ê³¼ëŠ” JSONìœ¼ë¡œ, keyëŠ” snake_case ì˜ë¬¸ í´ë”ëª…ì…ë‹ˆë‹¤.

ë¬¸ì„œ ì œëª©:
{chr(10).join(names)}
"""
    r = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
    )
    data = json.loads(r["choices"][0]["message"]["content"])
    sub_cache[k] = data
    save(CACHE / "sub.json", sub_cache)
    return data

def generate_readme(title, files):
    k = h(lang + title + "||".join(files))
    if k in readme_cache:
        return readme_cache[k]

    lang_rule = "ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”." if lang == "í•œêµ­ì–´" else "Write in English."

    prompt = f"""
'{title}' í´ë”ì— ëŒ€í•œ README.mdë¥¼ ì‘ì„±í•˜ì„¸ìš”.
{lang_rule}

íŒŒì¼:
{chr(10).join(files)}
"""
    r = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": (
                    "Folder names are English for system use, "
                    "but README language must follow the instruction."
                ),
            },
            {"role": "user", "content": prompt},
        ],
    )
    text = r["choices"][0]["message"]["content"]
    readme_cache[k] = text
    save(CACHE / "readme.json", readme_cache)
    return text

# ----------------------------
# ğŸš€ ë©”ì¸
# ----------------------------
if uploaded_files:
    uploaded_files = [f for f in uploaded_files if f.name.strip()]
    progress = progress_placeholder.progress(0)

    titles = [f"title: {f.name.split('.')[0]}" for f in uploaded_files]
    labels = HDBSCAN(min_cluster_size=2).fit_predict(embed_titles(titles))

    groups = {}
    for f, l in zip(uploaded_files, labels):
        groups.setdefault(l, []).append(f)

    out = Path("output_docs")
    out.mkdir(exist_ok=True)

    total = len(groups)
    done = 0

    for label, files in groups.items():
        if label == -1:
            group = "ë¯¸ë¶„ë¥˜_ë¬¸ì„œ" if lang == "í•œêµ­ì–´" else "unclassified_documents"
        else:
            group = generate_group_name([f.name.split(".")[0] for f in files])

        gdir = out / group
        gdir.mkdir(exist_ok=True)

        subgroups = generate_subgroups([f.name.split(".")[0] for f in files])

        for sub, names in subgroups.items():
            sdir = gdir / sub
            sdir.mkdir(exist_ok=True)

            sub_files = []
            for f in files:
                if f.name.split(".")[0] in names:
                    (sdir / f.name).write_bytes(f.getvalue())
                    sub_files.append(f.name)

            (sdir / "README.md").write_text(
                generate_readme(sub, sub_files),
                encoding="utf-8",
            )

        (gdir / "README.md").write_text(
            generate_readme(group, [f.name for f in files]),
            encoding="utf-8",
        )

        done += 1
        progress.progress(int(done / total * 100))
        progress_text.markdown(
            f"<div class='status-bar'>[{done}/{total} ì²˜ë¦¬ ì™„ë£Œ]</div>",
            unsafe_allow_html=True,
        )
        log(f"{group} ì™„ë£Œ")

    with zipfile.ZipFile("result_documents.zip", "w") as z:
        for root, _, fs in os.walk(out):
            for f in fs:
                p = os.path.join(root, f)
                z.write(p, arcname=os.path.relpath(p, out))

    zip_placeholder.download_button(
        "ğŸ“¥ ì •ë¦¬ëœ ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
        open("result_documents.zip", "rb"),
        file_name="result_documents.zip",
        mime="application/zip",
    )

    progress.progress(100)
    progress_text.markdown(
        "<div class='status-bar'>[100% ì™„ë£Œ]</div>",
        unsafe_allow_html=True,
    )

else:
    progress_placeholder.progress(0)
    progress_text.markdown(
        "<div class='status-bar'>[ëŒ€ê¸° ì¤‘]</div>",
        unsafe_allow_html=True,
    )
