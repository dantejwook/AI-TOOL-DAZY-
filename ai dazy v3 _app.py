import streamlit as st
import zipfile
import os
from pathlib import Path
import openai
from hdbscan import HDBSCAN
import json
import hashlib
import re

# ============================
# ğŸ”§ ë¶„ë¥˜ / ì¬ë¶„í•´ / ì‹œê°„ ì„¤ì •
# ============================
MAX_FILES_PER_CLUSTER = 25
MAX_RECURSION_DEPTH = 2
AUTO_SPLIT_NOTICE = "âš ï¸ ì´ í´ë”ëŠ” íŒŒì¼ ìˆ˜ ì œí•œ(25ê°œ)ìœ¼ë¡œ ì¸í•´ ìë™ ë¶„í•´ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"

# ì •ë°€ë„ë³„ HDBSCAN íŒŒë¼ë¯¸í„°
CLUSTER_PARAMS = {
    1: {"min_cluster_size": 8, "min_samples": 1},  # ëŠìŠ¨
    2: {"min_cluster_size": 5, "min_samples": 1},  # ê¸°ë³¸
    3: {"min_cluster_size": 3, "min_samples": 2},  # íƒ€ì´íŠ¸
}

# ì‹œê°„ ì˜ˆì¸¡ìš© í‰ê· ê°’ (ì´ˆ)
AVG_EMBED_SEC_PER_FILE = 0.03
AVG_README_SEC_MIN = 1.2
AVG_README_SEC_MAX = 2.0

# ----------------------------
# ğŸŒˆ ê¸°ë³¸ í˜ì´ì§€ ì„¤ì •
# ----------------------------
st.set_page_config(page_title="AI dazy document sorter", page_icon="ğŸ—‚ï¸", layout="wide")

# ----------------------------
# ğŸ” OpenAI API í‚¤ ì„¤ì •
# ----------------------------
openai.api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
if not openai.api_key:
    st.sidebar.error("ğŸš¨ OpenAI API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()
else:
    st.sidebar.success("âœ… OpenAI Key ë¡œë“œ ì™„ë£Œ")

# ----------------------------
# ğŸ¨ ìŠ¤íƒ€ì¼ (ê¸°ì¡´ ìœ ì§€)
# ----------------------------
st.markdown(
    """
    <style>
    body { background-color: #f8f9fc; font-family: 'Pretendard', sans-serif; }
    .stButton>button {
        border-radius: 10px; background-color: #4a6cf7; color: white;
        border: none; padding: 0.6em 1.2em; font-weight: 600;
    }
    .stButton>button:hover { background-color: #3451c1; }
    .status-bar {
        background-color: #595656; border-radius: 6px;
        padding: 0.5em; margin-top: 20px; font-size: 0.9em;
    }
    .log-box {
        background-color: #595656; border-radius: 6px;
        padding: 0.8em; margin-top: 10px;
        height: 120px; overflow-y: auto; font-size: 0.85em;
        border: 1px solid #dee2e6;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# ----------------------------
# ğŸ§­ ì‚¬ì´ë“œë°”
# ----------------------------
st.sidebar.title("âš™ï¸ ì„¤ì •")

if st.sidebar.button("ğŸ” ë‹¤ì‹œ ì‹œì‘"):
    st.markdown("<script>window.location.reload();</script>", unsafe_allow_html=True)

lang = st.sidebar.selectbox("ğŸŒ ì–¸ì–´ ì„ íƒ", ["í•œêµ­ì–´", "English"])

invalidate_cache = st.sidebar.checkbox(
    "â™»ï¸ ì •ë°€ë„ ë³€ê²½ ì‹œ ìºì‹œ ì´ˆê¸°í™”",
    value=False,
    help="ê·¸ë£¹ëª… / README ìºì‹œë§Œ ì´ˆê¸°í™” (ì„ë² ë”© ìœ ì§€)"
)

if "tightness" not in st.session_state:
    st.session_state.tightness = 2

tightness = st.sidebar.slider("ğŸ“Š ë¶„ë¥˜ ì •ë°€ë„", 1, 3, st.session_state.tightness)

if tightness != st.session_state.tightness:
    st.session_state.tightness = tightness
    if invalidate_cache:
        Path(".cache/group_names.json").unlink(missing_ok=True)
        Path(".cache/readmes.json").unlink(missing_ok=True)

if st.sidebar.button("ğŸ¤– ìë™ ì¶”ì²œ ì •ë°€ë„"):
    file_count = len(st.session_state.get("uploaded_files", []))
    if file_count <= 30:
        st.session_state.tightness = 3
    elif file_count <= 80:
        st.session_state.tightness = 2
    else:
        st.session_state.tightness = 1
    st.rerun()

# ----------------------------
# ğŸ“ ë©”ì¸ UI
# ----------------------------
left_col, right_col = st.columns([1, 1])

with left_col:
    st.subheader("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader(
        "ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (.md, .pdf, .txt)",
        accept_multiple_files=True,
        type=["md", "pdf", "txt"],
    )

if uploaded_files:
    uploaded_files = [f for f in uploaded_files if f and f.name.strip()]
    st.session_state.uploaded_files = uploaded_files
    if not uploaded_files:
        st.error("â— ìœ íš¨í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

with right_col:
    st.subheader("ğŸ“¦ ZIP ë‹¤ìš´ë¡œë“œ")
    zip_placeholder = st.empty()

# ----------------------------
# âš™ï¸ ìƒíƒœ / ë¡œê·¸
# ----------------------------
progress_placeholder = st.empty()
progress_text = st.empty()
log_box = st.empty()
logs = []

def log(msg):
    logs.append(msg)
    log_box.markdown(
        "<div class='log-box'>" + "<br>".join(logs[-10:]) + "</div>",
        unsafe_allow_html=True,
    )

# ----------------------------
# ğŸ§  ìºì‹œ
# ----------------------------
CACHE_DIR = Path(".cache")
CACHE_DIR.mkdir(exist_ok=True)

def load_cache(p):
    try:
        return json.loads(p.read_text()) if p.exists() else {}
    except Exception:
        return {}

def save_cache(p, d):
    p.write_text(json.dumps(d, ensure_ascii=False, indent=2))

EMBED_CACHE = CACHE_DIR / "embeddings.json"
GROUP_CACHE = CACHE_DIR / "group_names.json"
README_CACHE = CACHE_DIR / "readmes.json"

embedding_cache = load_cache(EMBED_CACHE)
group_cache = load_cache(GROUP_CACHE)
readme_cache = load_cache(README_CACHE)

def h(t): return hashlib.sha256(t.encode("utf-8")).hexdigest()

# ----------------------------
# âœ¨ ìœ í‹¸
# ----------------------------
def sanitize_folder_name(name):
    name = re.sub(r"[^\wê°€-í£\s]", "", name)
    name = re.sub(r"\s+", "_", name)
    return name.strip("_") or "ê¸°íƒ€_ë¬¸ì„œ"

def unique_folder_name(base, used):
    if base not in used:
        return base
    i = 1
    while f"{base}_{i}" in used:
        i += 1
    return f"{base}_{i}"

# ----------------------------
# âœ¨ ì„ë² ë”© / í´ëŸ¬ìŠ¤í„°
# ----------------------------
def embed_titles(titles):
    missing = [t for t in titles if h(t) not in embedding_cache]
    if missing:
        r = openai.Embedding.create(
            model="text-embedding-3-large",
            input=missing,
        )
        for t, d in zip(missing, r["data"]):
            embedding_cache[h(t)] = d["embedding"]
        save_cache(EMBED_CACHE, embedding_cache)
    return [embedding_cache[h(t)] for t in titles]

def cluster_documents(files):
    titles = [f"title: {f.name.split('.')[0]}" for f in files]
    vectors = embed_titles(titles)
    params = CLUSTER_PARAMS[st.session_state.tightness]
    return HDBSCAN(
        min_cluster_size=params["min_cluster_size"],
        min_samples=params["min_samples"],
    ).fit_predict(vectors)

# ----------------------------
# ğŸ” ìë™ ì¬ë¶„í•´
# ----------------------------
def recursive_cluster(files, depth=0):
    if len(files) <= MAX_FILES_PER_CLUSTER or depth >= MAX_RECURSION_DEPTH:
        return [files]

    labels = cluster_documents(files)
    groups = {}
    for f, l in zip(files, labels):
        groups.setdefault(l, []).append(f)

    result = []
    for g in groups.values():
        if len(g) > MAX_FILES_PER_CLUSTER:
            result.extend(recursive_cluster(g, depth + 1))
        else:
            result.append(g)

    final = []
    for g in result:
        if len(g) > MAX_FILES_PER_CLUSTER:
            for i in range(0, len(g), MAX_FILES_PER_CLUSTER):
                final.append(g[i:i + MAX_FILES_PER_CLUSTER])
        else:
            final.append(g)
    return final

# ----------------------------
# ğŸ“Š ì˜ˆìƒ ê²°ê³¼ & ì‹œê°„ ë¯¸ë¦¬ë³´ê¸°
# ----------------------------
if uploaded_files:
    st.sidebar.markdown("### ğŸ” ì˜ˆìƒ ë¶„ë¥˜ ê²°ê³¼")
    titles = [f"title: {f.name.split('.')[0]}" for f in uploaded_files]
    vectors = embed_titles(titles)
    file_count = len(uploaded_files)
    embed_time = file_count * AVG_EMBED_SEC_PER_FILE

    for lvl, label in [(1, "ëŠìŠ¨"), (2, "ê¸°ë³¸"), (3, "íƒ€ì´íŠ¸")]:
        p = CLUSTER_PARAMS[lvl]
        labels = HDBSCAN(
            min_cluster_size=p["min_cluster_size"],
            min_samples=p["min_samples"],
        ).fit_predict(vectors)
        folder_count = len(set(labels)) - (1 if -1 in labels else 0)
        folder_count = max(folder_count, 1)

        readme_count = folder_count * 2
        min_t = embed_time + readme_count * AVG_README_SEC_MIN
        max_t = embed_time + readme_count * AVG_README_SEC_MAX

        st.sidebar.write(
            f"{label}: ì•½ {folder_count}ê°œ / {int(min_t)}~{int(max_t)}ì´ˆ"
        )

# ----------------------------
# ğŸš€ ë©”ì¸ ì²˜ë¦¬
# ----------------------------
if uploaded_files:
    progress = progress_placeholder.progress(0)
    progress_text.markdown("<div class='status-bar'>[0%]</div>", unsafe_allow_html=True)
    log("íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ")

    output_dir = Path("output_docs")
    output_dir.mkdir(exist_ok=True)

    top_clusters = recursive_cluster(uploaded_files)
    total = len(top_clusters)
    done = 0

    for cluster_files in top_clusters:
        auto_split = len(cluster_files) > MAX_FILES_PER_CLUSTER
        main_group = sanitize_folder_name(
            cluster_files[0].name.split(".")[0]
        )
        main_group = generate_group = None

        main_group = sanitize_folder_name(
            generate_group_name([f.name.split(".")[0] for f in cluster_files])
        )

        main_folder = output_dir / main_group
        main_folder.mkdir(parents=True, exist_ok=True)

        readme = AUTO_SPLIT_NOTICE if auto_split else ""
        readme += generate_readme(main_group, [f.name for f in cluster_files])
        (main_folder / "README.md").write_text(readme, encoding="utf-8")

        sub_clusters = recursive_cluster(cluster_files)
        used = set()

        for sub_files in sub_clusters:
            base = sanitize_folder_name(
                generate_group_name([f.name.split(".")[0] for f in sub_files])
            )
            sub_group = unique_folder_name(base, used)
            used.add(sub_group)

            sub_folder = main_folder / sub_group
            sub_folder.mkdir(parents=True, exist_ok=True)

            for f in sub_files:
                (sub_folder / f.name).write_bytes(f.getvalue())

            sub_readme = AUTO_SPLIT_NOTICE if len(sub_files) >= MAX_FILES_PER_CLUSTER else ""
            sub_readme += generate_readme(
                f"{main_group} - {sub_group}",
                [f.name for f in sub_files],
            )
            (sub_folder / "README.md").write_text(sub_readme, encoding="utf-8")

        done += 1
        pct = int(done / total * 100)
        progress.progress(pct)
        progress_text.markdown(
            f"<div class='status-bar'>[{pct}% ({done}/{total})]</div>",
            unsafe_allow_html=True,
        )
        log(f"{main_group} ì²˜ë¦¬ ì™„ë£Œ")

    with zipfile.ZipFile("result_documents.zip", "w") as z:
        for root, _, files in os.walk(output_dir):
            for f in files:
                p = os.path.join(root, f)
                z.write(p, arcname=os.path.relpath(p, output_dir))

    zip_placeholder.download_button(
        "ğŸ“¥ ì •ë¦¬ëœ ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
        open("result_documents.zip", "rb"),
        file_name="result_documents.zip",
        mime="application/zip",
    )

    progress.progress(100)
    progress_text.markdown(
        "<div class='status-bar'>[100% complete]</div>",
        unsafe_allow_html=True,
    )
    log("ëª¨ë“  ë¬¸ì„œ ì •ë¦¬ ì™„ë£Œ")

else:
    progress_placeholder.progress(0)
    progress_text.markdown(
        "<div class='status-bar'>[ëŒ€ê¸° ì¤‘]</div>",
        unsafe_allow_html=True,
    )
    log_box.markdown("<div class='log-box'>ëŒ€ê¸° ì¤‘...</div>", unsafe_allow_html=True)
